{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xor data\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning (one hidden layer)\n",
    "- from 정석으로 배우는 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "tf.set_random_seed(1234)\n",
    "in_dim=2\n",
    "x=tf.placeholder(tf.float32,shape=[None,in_dim]) #input 2개\n",
    "t=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "#input layer-hidden layer\n",
    "W=tf.Variable(tf.truncated_normal([in_dim,2], seed=0)) \n",
    "#\n",
    "b=tf.Variable(tf.zeros([2])) #(0,0), 2rㅐ\n",
    "h=tf.nn.sigmoid(tf.matmul(x,W)+b)\n",
    "\n",
    "#hidden layer-output layer\n",
    "V=tf.Variable(tf.truncated_normal([in_dim,1],seed=23))\n",
    "c=tf.Variable(tf.zeros([1])) #(0)\n",
    "y=tf.nn.sigmoid(tf.matmul(h,V)+c)\n",
    "\n",
    "#loss function\n",
    "cross_entropy=-tf.reduce_mean(t*tf.log(y)+(1-t)*tf.log(1-y))\n",
    "\n",
    "#optimizer\n",
    "lr=0.1\n",
    "optimizer=tf.train.GradientDescentOptimizer(lr)\n",
    "train_step=optimizer.minimize(cross_entropy)\n",
    "\n",
    "#evaluation\n",
    "correct_prediction=tf.equal(tf.to_float(tf.greater(y,0.5)),t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= [[-0.30200204  1.9954948 ]\n",
      " [-1.4346613   0.05055132]]\n",
      "V= [[-1.3118125 ]\n",
      " [ 0.15874536]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "print('W=', sess.run(W))\n",
    "print('V=',sess.run(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1000\n",
      "epoch: 2000\n",
      "epoch: 3000\n",
      "epoch: 4000\n",
      "epoch: 5000\n",
      "epoch: 6000\n",
      "epoch: 7000\n"
     ]
    }
   ],
   "source": [
    "#Learning\n",
    "for epoch in range(8000):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x:X,\n",
    "        t:Y\n",
    "    })\n",
    "    if epoch % 1000==0:\n",
    "        print('epoch:', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified: [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "output probability: [[0.01746599]\n",
      " [0.97482604]\n",
      " [0.9751473 ]\n",
      " [0.02449046]]\n"
     ]
    }
   ],
   "source": [
    "# 결과확인\n",
    "classified=correct_prediction.eval(session=sess, feed_dict={\n",
    "    x:X,\n",
    "    t:Y\n",
    "})\n",
    "prob=y.eval(session=sess, feed_dict={\n",
    "    x:X\n",
    "})\n",
    "\n",
    "print('classified:',classified)\n",
    "print('output probability:', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification(IRIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n",
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris=load_iris()\n",
    "\n",
    "print(iris.keys())\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "x=iris.data\n",
    "y=iris.target\n",
    "target_names=iris.target_names\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(112,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y,random_state=10, test_size=0.25)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- change formation for output(multi-class classfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 35 38]\n"
     ]
    }
   ],
   "source": [
    "count_train=np.bincount(y_train)\n",
    "print(count_train) #0:39, 1:35, 2:38                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4) (112, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train=np.eye(3)[y_train].astype(int) #target identity matrix로 바꾸는 과정\n",
    "Y_test=np.eye(3)[y_test].astype(int)\n",
    "\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDim=4 #'data', 'target', 'target_names','feature_names']\n",
    "outDim=3 #0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "hiddenDim=10\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=[None,inDim])\n",
    "t=tf.placeholder(tf.float32, shape=[None,outDim])\n",
    "\n",
    "#input layer-hidden layer\n",
    "W=tf.Variable(tf.truncated_normal([inDim, hiddenDim],seed=0))\n",
    "b=tf.Variable(tf.zeros([hiddenDim])) #0~0.1사이의 작은값\n",
    "h=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "#hidden layer-output layer\n",
    "V=tf.Variable(tf.truncated_normal([hiddenDim,outDim],seed=23))\n",
    "c=tf.Variable(tf.zeros([outDim]))\n",
    "y=tf.nn.softmax(tf.matmul(h,V)+c)\n",
    "\n",
    "#Loss function\n",
    "cross_entropy=-tf.reduce_mean(tf.reduce_sum(t*tf.log(y),reduction_indices=[1])) #reduce sum 하는 축\n",
    "\n",
    "#traning\n",
    "lr=0.05 #learning rate\n",
    "optimizer=tf.train.GradientDescentOptimizer(lr)\n",
    "train_step=optimizer.minimize(cross_entropy)\n",
    "\n",
    "#evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(y,1), tf.argmax(t,1)) #argmax:결과 값중 최댓값의 인덱스를 얻을 때\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all variables\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set loss= 1.3322313 ,train_set accuracy= 0.3392857\n"
     ]
    }
   ],
   "source": [
    "#Before training\n",
    "loss_train=cross_entropy.eval(session=sess, feed_dict={\n",
    "    x:X_train, \n",
    "    t:Y_train\n",
    "})\n",
    "\n",
    "accuracy_train=accuracy.eval(session=sess, feed_dict={x:X_train, t:Y_train})\n",
    "\n",
    "print(\"train_set loss=\",loss_train,\n",
    "     \",train_set accuracy=\", accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss of training set= 1.3222893 accuracy of training set= 0.3482143\n",
      "epoch: 1000 loss of training set= 1.0975782 accuracy of training set= 0.3482143\n",
      "epoch: 2000 loss of training set= 1.0975776 accuracy of training set= 0.3482143\n",
      "epoch: 3000 loss of training set= 1.097577 accuracy of training set= 0.3482143\n",
      "epoch: 4000 loss of training set= 1.0975764 accuracy of training set= 0.3482143\n",
      "epoch: 5000 loss of training set= 1.097576 accuracy of training set= 0.3482143\n",
      "epoch: 6000 loss of training set= 1.0975755 accuracy of training set= 0.3482143\n",
      "epoch: 7000 loss of training set= 1.0975751 accuracy of training set= 0.3482143\n",
      "epoch: 8000 loss of training set= 1.0975748 accuracy of training set= 0.3482143\n",
      "epoch: 9000 loss of training set= 1.0975745 accuracy of training set= 0.3482143\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "n_epoch=1000\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x:X_train,\n",
    "        t:Y_train\n",
    "    })\n",
    "    if epoch % 100==0:\n",
    "        loss_train=cross_entropy.eval(session=sess,feed_dict={\n",
    "            x:X_train,\n",
    "            t:Y_train\n",
    "        })\n",
    "        \n",
    "        accurcy_train=accuracy.eval(session=sess, feed_dict={\n",
    "            x:X_train,\n",
    "            t:Y_train\n",
    "        })\n",
    "        \n",
    "        print('epoch:', epoch,\n",
    "             'loss of training set=',loss_train,\n",
    "             \"accuracy of training set=\",accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set loss= 1.0975741 , train_set accuracy= 0.3482143\n",
      "[False False  True  True False  True False  True False False False False\n",
      " False False False False False  True  True False  True False  True  True\n",
      " False False False  True False  True False False  True False False False\n",
      " False False  True False False False  True False False False  True  True\n",
      "  True False False False False  True  True  True False False False False\n",
      " False False False False False  True  True False  True  True False False\n",
      "  True  True  True False  True False  True False False  True False False\n",
      " False  True False  True False False False False  True False False False\n",
      " False False False  True False  True  True False  True False False False\n",
      " False  True False  True]\n"
     ]
    }
   ],
   "source": [
    "#For Train set\n",
    "loss_train=cross_entropy.eval(session=sess, feed_dict={\n",
    "    x:X_train,\n",
    "    t:Y_train\n",
    "})\n",
    "accuracy_train=accuracy.eval(session=sess, feed_dict={\n",
    "    x:X_train,\n",
    "    t:Y_train\n",
    "})\n",
    "\n",
    "print(\"train_set loss=\",loss_train,\n",
    "      \", train_set accuracy=\",accuracy_train)\n",
    "\n",
    "#y_val_train=y.eval(session=sess, feed_dict={\n",
    "#    x:X_train,\n",
    "#    t:Y_train\n",
    "#})\n",
    "#print(y_val_train)\n",
    "\n",
    "classified=correct_prediction.eval(session=sess, feed_dict={x:X_train, t:Y_train})\n",
    "\n",
    "print(classified)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set loss= 1.105864 ,test_set accuracy 0.28947368\n",
      "[False False  True False  True False False False  True False False False\n",
      " False  True  True False False  True  True  True False False False  True\n",
      " False  True False False False False False False False False False  True\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "#For the test set\n",
    "loss_test=cross_entropy.eval(session=sess, feed_dict={x:X_test, t:Y_test})\n",
    "accuracy_test=accuracy.eval(session=sess, feed_dict={x:X_test, t:Y_test})\n",
    "print(\"test_set loss=\", loss_test,\n",
    "      \",test_set accuracy\", accuracy_test)\n",
    "\n",
    "#y_val_test=y.eval(session=sess, feed_dict={\n",
    "#    x:X_test,\n",
    "#    t:Y_test\n",
    "#})\n",
    "#print(y_val_test)\n",
    "\n",
    "classified=correct_prediction.eval(session=sess,feed_dict={x:X_test,t:Y_test})\n",
    "print(classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
